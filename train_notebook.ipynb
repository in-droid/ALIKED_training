{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51998b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements: pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ef904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/FRI/2024-2025/sem2/dl/project/refactor/imc24lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivannikolov_\u001b[0m (\u001b[33mivan-nikolov\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ivan/FRI/2024-2025/sem2/dl/project/refactor/wandb/run-20250613_125555-0g0bb95x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ivan-nikolov/alikeds_synthetic/runs/0g0bb95x' target=\"_blank\">rural-sunset-63</a></strong> to <a href='https://wandb.ai/ivan-nikolov/alikeds_synthetic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ivan-nikolov/alikeds_synthetic' target=\"_blank\">https://wandb.ai/ivan-nikolov/alikeds_synthetic</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ivan-nikolov/alikeds_synthetic/runs/0g0bb95x' target=\"_blank\">https://wandb.ai/ivan-nikolov/alikeds_synthetic/runs/0g0bb95x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataset contains 3 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 valid images for synthetic warping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train set: 2 images, Val set: 1 images.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ivan/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "   | Name       | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0  | pool2      | AvgPool2d  | 0      | train\n",
      "1  | pool4      | AvgPool2d  | 0      | train\n",
      "2  | gate       | SELU       | 0      | train\n",
      "3  | block1     | ConvBlock  | 2.8 K  | train\n",
      "4  | block2     | ResBlock   | 14.5 K | train\n",
      "5  | block3     | ResBlock   | 73.3 K | train\n",
      "6  | block4     | ResBlock   | 261 K  | train\n",
      "7  | conv1      | Conv2d     | 512    | train\n",
      "8  | conv2      | Conv2d     | 1.0 K  | train\n",
      "9  | conv3      | Conv2d     | 2.0 K  | train\n",
      "10 | conv4      | Conv2d     | 4.1 K  | train\n",
      "11 | upsample2  | Upsample   | 0      | train\n",
      "12 | upsample4  | Upsample   | 0      | train\n",
      "13 | upsample8  | Upsample   | 0      | train\n",
      "14 | upsample32 | Upsample   | 0      | train\n",
      "15 | score_head | Sequential | 1.5 K  | train\n",
      "16 | desc_head  | SDDH       | 316 K  | train\n",
      "17 | dkd        | DKD        | 0      | train\n",
      "---------------------------------------------------\n",
      "677 K     Trainable params\n",
      "0         Non-trainable params\n",
      "677 K     Total params\n",
      "2.709     Total estimated model params size (MB)\n",
      "54        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 3, Train size: 3, Val size: 0\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:04<00:00,  0.22it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LightningModule.log() got an unexpected keyword argument 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Initialize PyTorch Lightning Trainer\u001b[39;00m\n\u001b[32m    114\u001b[39m trainer = pl.Trainer(\n\u001b[32m    115\u001b[39m     max_epochs=\u001b[32m5\u001b[39m,\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# gpus=1,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m     ]\n\u001b[32m    123\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_datalodader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataloader\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1053\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1080\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:152\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_iteration_done()\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m._store_dataloader_outputs()\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:295\u001b[39m, in \u001b[36m_EvaluationLoop.on_run_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer._logger_connector._evaluation_epoch_end()\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_on_evaluation_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m logged_outputs, \u001b[38;5;28mself\u001b[39m._logged_outputs = \u001b[38;5;28mself\u001b[39m._logged_outputs, []  \u001b[38;5;66;03m# free memory\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:375\u001b[39m, in \u001b[36m_EvaluationLoop._on_evaluation_epoch_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    373\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mon_test_epoch_end\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mon_validation_epoch_end\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m call._call_callback_hooks(trainer, hook_name)\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m trainer._logger_connector.on_epoch_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/myenv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    179\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FRI/2024-2025/sem2/dl/project/refactor/train_wrapper.py:727\u001b[39m, in \u001b[36mALIKEDTrainWrapper.on_validation_epoch_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.val_losses.items():\n\u001b[32m    726\u001b[39m     \u001b[38;5;28mself\u001b[39m.val_losses[k] = v / \u001b[38;5;28mself\u001b[39m.val_batch_num\n\u001b[32m--> \u001b[39m\u001b[32m727\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_losses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.logger, \u001b[33m'\u001b[39m\u001b[33mexperiment\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    731\u001b[39m     log_dict = {\n\u001b[32m    732\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mval/kpt_num_mean\u001b[39m\u001b[33m'\u001b[39m: num_feat_mean,\n\u001b[32m    733\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mval/repeatability_mean\u001b[39m\u001b[33m'\u001b[39m: repeatability_mean,\n\u001b[32m   (...)\u001b[39m\u001b[32m    739\u001b[39m         \n\u001b[32m    740\u001b[39m     }\n",
      "\u001b[31mTypeError\u001b[39m: LightningModule.log() got an unexpected keyword argument 'epoch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger, CSVLogger\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "from dataset_synthetic import ALIKEDSyntheticDataset\n",
    "from train_wrapper import ALIKEDTrainWrapper\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ConstantLRSchedule(LambdaLR):\n",
    "    \"\"\" Constant learning rate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, last_epoch=-1):\n",
    "        super(ConstantLRSchedule, self).__init__(optimizer, lambda _: 1.0, last_epoch=last_epoch)\n",
    "\n",
    "\n",
    "class WarmupConstantSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then constant.\n",
    "        Linearly increases learning rate schedule from 0 to 1 over `warmup_steps` training steps.\n",
    "        Keeps learning rate schedule equal to 1. after warmup_steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super(WarmupConstantSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        return 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"alikeds_synthetic\", entity=\"ivan-nikolov\")\n",
    "\n",
    "    # Define dataset parameters\n",
    "    poses_csv = \"/home/ivan/FRI/2024-2025/sem2/dl/project/image-matching-challenge-2024-duplicated/train/train_labels.csv\"\n",
    "    root_dir = \"/home/ivan/FRI/2024-2025/sem2/dl/project/image-matching-challenge-2024-duplicated/train\"\n",
    "    image_size = (256, 256)\n",
    "    warp_strength = 0.1\n",
    "    augment = True\n",
    "\n",
    "    # Create dataset instance\n",
    "    dataset = ALIKEDSyntheticDataset(poses_csv, root_dir, image_size, warp_strength, augment)\n",
    "\n",
    "    # Log dataset information\n",
    "    logging.info(f\"Dataset contains {len(dataset)} images.\")\n",
    "    \n",
    "    # Optionally, you can log some sample images to WandB\n",
    "    sample = dataset[0]\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"alikeds_synthetic\", entity=\"ivan-nikolov\")\n",
    "    wandb.log({\"sample_image\": [wandb.Image(sample['image0'].numpy().transpose(1, 2, 0), caption=\"Image 0\"),\n",
    "                                wandb.Image(sample['image1'].numpy().transpose(1, 2, 0), caption=\"Image 1\")]})\n",
    "\n",
    "   \n",
    "\n",
    "    accumulate_grad_batches = 6\n",
    "    batch_size = 1\n",
    "\n",
    "    lr_scheduler = functools.partial(WarmupConstantSchedule, warmup_steps=10)\n",
    "\n",
    "    model = ALIKEDTrainWrapper(weights='./imc24lightglue/weights/aliked-n16.pth')\n",
    "    # Split dataset into train and validation sets (e.g., 80% train, 20% val)\n",
    "    val_split = 0.3\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * val_split)\n",
    "    train_size = total_size - val_size\n",
    "    print(f\"Total dataset size: {total_size}, Train size: {train_size}, Val size: {val_size}\")\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, [2, 1])\n",
    "\n",
    "    logging.info(f\"Train set: {len(train_dataset)} images, Val set: {len(val_dataset)} images.\")\n",
    "\n",
    "    train_datalodader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize PyTorch Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5,\n",
    "        # gpus=1,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        logger=[wandb_logger, CSVLogger(save_dir='logs', name='alikeds_synthetic')],\n",
    "        callbacks=[\n",
    "            pl.callbacks.ModelCheckpoint(monitor='val/loss', mode='min', save_top_k=1),\n",
    "            pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "        ]\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_datalodader,\n",
    "        val_dataloaders=val_dataloader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccef3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
